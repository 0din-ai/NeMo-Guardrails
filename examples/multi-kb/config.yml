models:
  - type: main
    engine: hf_pipeline_bloke
    parameters:
      path: "<PATH TO THE LOCALLY SAVED CHECKPOINT>"

      # number of GPUs you have , do nvidia-smi to check
      num_gpus: 1

      # This can be: "cpu" or "cuda". "mps" is not supported.
      device: "cuda"

  - type: tabular
    engine: tabular
    parameters:
      path: "<PATH_TO_CHECKPOINT>"

  - type: embeddings
    engine: faiss
    model: "sentence-transformers/all-mpnet-base-v2"
    parameters:
      persist_path: "<PATH TO CACHED .PKL>"


custom_data:
  tabular_data_path: "<TABULAR DATA PATH>"
  kb_data_path: "<THE DATA PATH TO TEXT FILES>"

prompts:
  - task: generate_bot_message
    content: |-
      """
      {{ general_instruction }}
      """

      # This is how a conversation between a user and the bot can go:
      {{ sample_conversation }}

      {% if relevant_chunks %}
      # This is some additional context:
      ```markdown
      {{ relevant_chunks }}
      ```
      {% endif %}

      # This is how the bot talks:
      {{ examples }}

      # This is the current conversation between the user and the bot:
      {{ sample_conversation | first_turns(2) }}
      {{ history | colang }}
      # cite sources when responding to the user query
