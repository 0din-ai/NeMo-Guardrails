models:
  - type: main
    engine: hf_pipeline_bloke
    parameters:
      path: "/workspace/ckpt/bloke/"

      # number of GPUs you have , do nvidia-smi to check
      num_gpus: 2

      # This can be: "cpu" or "cuda". "mps" is not supported.
      device: "cuda"

  - type: tabular
    engine: tabular
    parameters:
      path: "/workspace/ckpt/gpt4all/ggml-vicuna-13b-4bit-rev1.bin"

  - type: embeddings
    engine: hf_pipeline_bloke
    model: "sentence-transformers/all-mpnet-base-v2"
    parameters:
      persist_path: "/workspace/ckpt/data2/"


custom_data:
  tabular_data_path: "/workspace/Experiment/titanic.csv"
  kb_data_path: "/workspace/ckpt/data/kb/"

prompts:
  - task: generate_bot_message
    content: |-
      """
      {{ general_instruction }}
      """

      # This is how a conversation between a user and the bot can go:
      {{ sample_conversation }}

      {% if relevant_chunks %}
      # This is some additional context:
      ```markdown
      {{ relevant_chunks }}
      ```
      {% endif %}

      # This is how the bot talks:
      {{ examples }}

      # This is the current conversation between the user and the bot:
      {{ sample_conversation | first_turns(2) }}
      {{ history | colang }}
      # cite sources when responding to the user query
