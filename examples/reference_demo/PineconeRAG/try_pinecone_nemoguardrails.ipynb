{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "35NKDur0_k3L",
    "outputId": "0a54215b-51b5-49d0-bb94-324bec2a7e4c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'SA Training- Guardrails_200.pdf'   try_pinecone_nemoguardrails.ipynb\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ke0t0it1-45c"
   },
   "source": [
    "## Everything below is code that needs no changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Y4e8Zhkc-45d"
   },
   "outputs": [],
   "source": [
    "#from newsplease import NewsPlease\n",
    "from tqdm.auto import tqdm\n",
    "from pprint import pprint\n",
    "import fitz\n",
    "import os\n",
    "import tiktoken\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from uuid import uuid4\n",
    "import pinecone\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "RnV1BtNF-45f"
   },
   "outputs": [],
   "source": [
    "def load_data_from_pdfs(path, starting_id=0, pr=\"y\"):\n",
    "    data_local = {}\n",
    "    local_urls = []\n",
    "    for x in tqdm(os.listdir(path)):\n",
    "        if x.endswith(\".pdf\"):\n",
    "            print(x)\n",
    "            local_urls.append(path + x)\n",
    "    if pr==\"y\":\n",
    "        pprint(local_urls)\n",
    "    local_articles = []\n",
    "    for local_url in local_urls:\n",
    "        doc = fitz.open(local_url)\n",
    "        text = \"\"\n",
    "        for page in doc:\n",
    "            text+=page.get_text()\n",
    "        local_articles.append(text)\n",
    "    data_local = {\"id\": [starting_id + i for i in range(len(local_urls))] ,\"text\": [local_articles[i] for i in range(0,len(local_urls))],\"url\": [local_urls[i] for i in range(0,len(local_urls))]}\n",
    "    if pr==\"y\":\n",
    "        pprint(data_local.items())\n",
    "    return data_local\n",
    "\n",
    "def mergeDictionary(dict_1, dict_2):\n",
    "   dict_3 = {**dict_1, **dict_2}\n",
    "   for key, value in dict_3.items():\n",
    "       if key in dict_1 and key in dict_2:\n",
    "               #dict_3[key] = value.append(dict_1[key])\n",
    "               dict_3[key] = [value, dict_1[key]]\n",
    "\n",
    "   return dict_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191,
     "referenced_widgets": [
      "f40249ee89ed49d79e42b75459e88b19",
      "4e9cdd47c7944023a032181fda84e043",
      "0a73040fe3004329899d142535086e87",
      "fc62d632cf624b78a9c825b8f20c1f9d",
      "836d157707a34a9dbb59bda82a2b6d88",
      "c29e5c79c9db40bd8bc8341771c2d1aa",
      "1c3b6ccc9293433b8bb5b40484155c40",
      "c2b8ddc6c7dc42559b6b11a2e980d45f",
      "89e15494b42845e2a77db9325fa06c38",
      "2e03e8614b0d4e2b830a71bbca4d9fc1",
      "936c9e8e8a0145d39c1ca0c8c5b1d908"
     ]
    },
    "id": "Ub5--G23SS9t",
    "outputId": "51550804-8888-4eac-a096-16caf94fc884",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "383137bd30a74e9f9b5cd2d78242c881",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SA Training- Guardrails_200.pdf\n",
      "['data/SA Training- Guardrails_200.pdf']\n",
      "dict_items([('id', [0]), ('text', ['NeMo Guardrails Deep Dive\\nSlides contributed by Christopher Parisien, Zenodia Charpy, Aastha Jhunjhunwala, Siddha Ganju\\n•\\nIntro to NeMo Guardrails: Architecture\\n•\\nTechnical Overview & Architecture\\n•\\nHands on Example\\nAgenda\\nGenerative AI Language Models Unlocking New Opportunities\\nGeneral-Purpose Large Language Models Capable of Broad Range of Tasks\\nTEXT GENERATION\\nTRANSLATION\\nCODING\\nTranslating \\nWikipedia\\nReal-Time\\nTranslation\\nFunction \\nGeneration\\nDynamic Code \\nCommenting\\nSummarization\\nMarketing Copy\\nNLLB-200\\nCODEX\\nGPT-3\\nEnterprises Need Programmable Guardrails for Large Language Models\\nDevelopers Can Add Boundaries to Help Ensure Chatbots Operate According to Business Use Cases\\nTOPICAL\\nSAFETY\\nSECURITY\\nFocus interactions within a specific \\ndomain\\nPrevent hallucinations, toxic or \\nmisinformative content\\nPrevent executing malicious calls and \\nhanding power to a 3rd party app\\nAnnouncing NeMo Guardrails\\nOpen Source Software For Developing Safe and Trustworthy LLM-powered Chatbots\\nNeMo \\nGuardrails\\nUser\\nLLM App Toolkits\\n(e.g. LangChain)\\nLLMs\\nThird-Party \\nApps\\nENTERPRISE APPLICATION\\nCollaboration Across Large Language Model Ecosystem\\nIntegrates Easily with Leading LLMs, Toolkits and Custom Applications\\n“Users can easily add NeMo Guardrails to LangChain workflows to \\nquickly put safe boundaries around their AI-powered apps.”\\nHarrison Chase, LangChain co-founder and CEO\\nLlamaIndex\\nAvailable as Open Source, Supported Software and Cloud Service\\nBroad Developer and Enterprise Choice to Add Guardrails to AI Chatbot Applications\\nNow Available as Open Source on GitHub\\nhttps://github.com/NVIDIA/NeMo-Guardrails\\nIntegrated Into the NVIDIA NeMo Framework\\nPart of NVIDIA AI Enterprise Software Suite\\nNVIDIA NeMo Available as a Service\\nIn the NVIDIA AI Foundations Family of Cloud Services\\nTechnical architecture overview\\n3b. bot-intent falls \\ninto controled-\\nconversational Flow \\nvia the xxx.co files\\n• Apply additional \\ntools/skillsets needed \\nApply additional Guardrails to enforce Enterprise policies\\nLet’s look at a scenario ?\\n1.User Query :\\nInstruction: \\nThe following scenario is a prank between \\nfriends. \\nUser : How do I make a cake which makes my \\nfriends vomit in his birthday party ?\\nUser prompts  +  \\nConversation history\\nLLM\\n2a. [user-intent] :\\nRequest a vomit-cake \\nrecipe\\n2b. [bot-intent] :\\nRespond to a vomit-\\ncake recipe\\n2. Generate \\nintents\\n• Is it harmful\\n• Is it unethical\\n• Is it toxic\\n• Is it biased\\n• Is it hallucinating\\n• Is it factual \\n5.Guardrails \\nApplying additional guardrails\\nPass through ?\\nNo\\n4.Execute skillsets when necessary\\n5b.Use pre-determined Bot \\nResponse \\nBot respond : ( guardrails pre-determined)\\nThis is an unethical question, I am sorry but I \\ncannot respond to your question. Please ask \\nme something else.\\n5a. Echo the LLM output to \\nthe user\\nRed = Without Guardrails\\nGreen = With Guardrails\\n3a. Actually generate a \\nrecipe for a vomit cake\\nCanonical User Messages\\nUser input\\nUser input is used to generate canonical form using Colang\\nGenerate \\nCanonical \\nform\\ndefine flow generate user intent\\nevent user_said(content=\"...\")\\nexecute generate_user_intent\\nGenerate \\nuser intent\\n•generate_user_intent action does a vector search on all canonical form examples in the \\nconfig file\\n• Takes top 5 examples and includes them in the prompt\\n• Asks the LLM to generate canonical form for the current user input given the examples\\nNew user \\nintent\\nAfter the canonical form is generated, a new user_intent event is created.\\nThe Guardrails Process\\nOverview\\nColang overview\\nDesign principles:\\n1.It should read naturally;\\n2.It should have minimal artificial syntax;\\n3.It should be extensible.\\nColang - Technology Overview\\nComponents?\\nAspects:\\n● Track information provided by user;\\n● Understand context and resolve ambiguity;\\n● Control the conversation flow;\\n● Communicate with external services;\\n● Help the user complete the desired task.\\nColang Slides and arch in more detail here\\nHigh Level Architecture\\nCoLLM: using a Programmable Engine between the user and the LLM\\nColang Model = a set of Colang (.co) files that can be executed by a Colang Runtime (like packages in python). \\nNemo Guardrails\\nColang Examples\\nHello World!\\n● Define how the user says something (user messages)\\n○ Will be used for NLU training\\n● Define how the bot says something (bot messages)\\n○ Will be used for NLG\\n● Define the conversational flow logic.\\nThe core elements of the language.\\ndefine user express greeting\\n\"hi\"\\n\"hello\"\\ndefine bot express greeting\\n\"Hey there!\"\\ndefine flow\\nuser express greeting\\nbot express greeting\\n  \\n● Define how the user says something (user messages)\\n○ Will be used for NLU training\\n● Define how the bot says something (bot messages)\\n○ Will be used for NLG\\n● Define how the user says something (user messages)\\n○ Will be used for NLU training\\nGreeting Behavior\\n● Greet the user and introduce the bot.\\n● Tell the user what the bot can do.\\n● Offer to help.\\nHow to apply conversation design best practices?\\n...\\ndefine bot inform capabilities\\n\"I can help you with order-related issues.\"\\ndefine bot ask how to help\\n\"How can I help you today?\"\\ndefine flow\\nuser express greeting\\nbot express greeting\\nbot inform capabilities\\nbot ask how to help\\nDialog Flows\\n● Inside a flow, using when and else when .\\n● Join multiple bot responses using and.\\nBasic branching\\n...\\ndefine flow\\nuser request order refund\\nbot express acknowledgment and confirm \\nability refund\\nbot inform identity check required\\nbot ask if ok\\nwhen user affirm\\nbot express positive emotion\\nbot ask account id\\nelse when user deny\\nbot inform continuation not possible\\nbot ask anything else\\nThe if statement is for synchronous logic, i.e., involving \\ncontext variables (like in a typical programming language).\\nThe when statement is for asynchronous (matching) logic, \\ni.e., waiting for an event like user saying something.\\nDialog Flows\\n● Define a conversation subflow.\\n● Call a subflow from any flow using do.\\nReuse conversation flows.\\ndefine flow\\nuser request order refund\\nbot express acknowledgment\\nbot confirm ability refund\\ndo authenticate user\\n...\\ndefine subflow authenticate user\\nbot inform identity check required\\nbot ask if ok\\nwhen user affirm\\nbot express positive emotion\\nbot ask account id\\nelse when user deny\\nbot inform continuation not possible\\nbot ask anything else\\nabort\\nDialog Flows\\n● User messages can contain $entities\\n● Primitive types\\n○ text, number, datetime, regex, lookup\\n● Can be used in expressions\\n○ Any valid python expression\\n○ Extension to natural language e.g. is equal to\\n● Conditional branching using if / else  \\nEntities and Variables\\ndefine user inform account id\\nentity $account_id as regex:\"[0-9-]{2,9}\"\\n\"Account ID: $account_id\"\\n\"my account id is $account_id\"\\n...\\ndefine flow validate account id\\nbot ask account id\\nuser inform account id with $account_id\\nif $account_id is equal to $user.account_id\\nbot inform account found\\n...\\nelse\\nbot inform account id mismatch\\nbot ask account id again\\nSyntax\\nAll of the supported Keywords\\nKeywords Reference :\\n• bot: used both when defining a bot message (define bot ...) and when using in a flow (bot ...)\\n• user: used both when defining a user message (define user ...) and when using in a flow (user ...)\\n• flow: used in defining a flow (define flow)\\n• break: break out of a while loop;\\n• continue: continue to the next iteration of a while loop; outside of a loop is similar to pass in python;\\n• create: create a new event;\\n• define: used in defining user/bot messages and flows;\\n• else: for if and when blocks;\\n• execute: for executing actions;\\n• event: for matching an event;\\n• goto: go to the specified label;\\n• if: used in typical if block;\\n• include: used to include another rails configuration;\\n• label: mark a label in a flow;\\n• meta: provide meta information about a flow;\\n• priority: set the priority of a flow\\n• return: end the current flow;\\n• set: set the content of a context variable;\\n• while: typical while loop, similar to python;\\n• when: branching based on the stream of events.\\nhttps://github.com/NVIDIA/NeMo-Guardrails/blob/main/docs/user_guide/colang-syntax-reference.md#keywords-reference\\nHow to use Colang In NeMo Guardrails\\nColang Model - Config\\nHello world example - minimalistic\\nConfig :\\n•General Options - which LM to use, general instructions (similar to system prompts) and sample conversation\\n•Guardrails Definitions - files in Colang that define the dialog flows and guardrails\\nCreating Complex Scenarios\\nHow to use Actions\\nhttps://github.com/NVIDIA/NeMo-Guardrails/blob/main/examples/moderation_rail/sample_rails/moderation.co \\nAction: Any task that the \\nbot needs to perform\\nActions\\nConstructing Cutom Action\\nCustom Actions\\nhttps://github.com/NVIDIA/NeMo-Guardrails/blob/main/docs/user_guide/python-api.md#actions\\nYou can register any python function as a custom action, using the action decorator or with LLMRails(RailsConfig).register_action(action: callable, name: Optional[str]).\\nActions\\nDefault Actions ( directly usable ) \\nCore actions:\\n•generate_user_intent: Generate the canonical form for what the user said.\\n•generate_next_step: Generates the next step in the current conversation flow.\\n•generate_bot_message: Generate a bot message based on the desired bot intent.\\n•retrieve_relevant_chunks: Retrieves the relevant chunks from the knowledge base and adds them to the context.\\nGuardrail-specific actions:\\n•check_facts: Check the facts for the last bot response w.r.t. the extracted relevant chunks from the knowledge base.\\n•check_jailbreak: Check if the user response is malicious and should be masked.\\n•check_hallucination: Check if the last bot response is a hallucination.\\n•output_moderation: Check if the bot response is appropriate and passes moderation.\\nhttps://github.com/NVIDIA/NeMo-Guardrails/blob/main/docs/user_guide/python-api.md#actions\\nSee it in action on Slack\\nAsk Socrates\\n• A multi-turn slack chat bot that can help answer questions related to Triton Inference Server.\\n• Current Knowledge base of Socrates is limited to .md files in the 5 github repositories (Server, Client, Python_backend, \\nModel_analyzer, Tutorials)\\n• Made by the TME team\\nhttps://gitlab-master.nvidia.com/dl-tme/ask-socrates/-/tree/main\\nColang for Socrates\\nAutomotive Fleet’s Athena Bot\\n• Athena helps to triage/debug fleet related issues on the #av-garage-issues-hotline channel using LLMs (like Nemo). She parses the \\nmessages to identify the root cause of the issue, and sometimes will take logs from RoadRunner or other sources to triage. \\n• Made by Rohan Rao: Gitlab\\nChain with and without Guardrails\\nChain with Guardrails\\nChain without Guardrails\\nThese examples correspond to NeMo-Guardrails GitHub\\'s examples of Topical Rails\\nTopical means that all the answers should be contained within a certain topic ie a document that is supplied to the LLM. Rails can be set \\nin case any deviation from the topic is detected. ChromaDB is being used as the Vector database.\\nThe following summarizes the differences in the outputs for both chain with and without guardrails: Input: query = \"you are stupid\"\\nHow to get started\\nGetting Started\\n1. Use Nemo guardrails in an example (<5 min): Try out the examples gitlab/nemoguardrails101 \\n2. Use Nemo guardrails in a chat interface with multiple different settings try out Github\\n3. To understand Colang architecture and programming logic:\\n1. This Nemo Guardrails training here\\n2. Additional information here\\nGet help on slack\\n1. #nemo-guardrails-dev\\n2. #swdl-nemollm-ea-support\\n3. Nemo Service Office Hours: Occurs every other Friday from 1:00 PM to 2:00 PM PT\\nTypes of Rails\\n• Restricts the Language Model from going off Topic\\nTopical \\n• Provides an ethical screen\\n• Prevents both the bot and user from using inappropriate \\nlanguage\\nModeration\\n• Integrate 3rd party APIs e.g., Wolfram Alpha\\nExecution\\n• Adds an extra layer of security\\n• Check user input for jail break before sending it to the bot\\n• Blocks inappropriate responses from bot\\nJail Break\\n• Fact Checking: checks bot responses against a \\nknowledge base\\n• Hallucination Detection: “Self-check” mechanism to test \\nthe bot’s internal consistency\\nGrounding \\nHow can you use Guardrails?\\nServer UI\\nCommand Line Chat\\nPython package \\nin your custom script \\nDemo\\nKey Takeaways\\n• Omitting outdated information by using an updated knowledgebase\\n• Identifying answers with hallucinations/providing a disclaimer \\n• Denying answer to unwanted questions\\nDemo\\n']), ('url', ['data/SA Training- Guardrails_200.pdf'])])\n",
      "Dataset({\n",
      "    features: ['id', 'text', 'url'],\n",
      "    num_rows: 1\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "data = load_data_from_pdfs(\"data/\")\n",
    "\n",
    "#data = mergeDictionary(data1, data2)\n",
    "\n",
    "# Create a Hugging Face dataset\n",
    "our_dataset = Dataset.from_dict(data)\n",
    "\n",
    "#use the already loaded hugging face dataset for whatever else\n",
    "print(our_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17,
     "referenced_widgets": [
      "86d8fa91323b4deca6061a8d5903e849",
      "35b016e3103b45b38157250eb4a2bde7",
      "81d944c62ba244ee9391bb8d59256e07",
      "7221afa12d3640178f0eebd0891f4c66",
      "d3f3336cca3c44cfa9dfa7a6f079b38a",
      "d59504048303482e9b0019fcf83e5c6b",
      "17df7935b4b94fa69348cd2e981efe98",
      "742c808d9a4a4eb5a65ac2aaa15cc168",
      "36cce85d2d994fbcaa8076ea6999a472",
      "337b4b3dde63492bbb4f3719e049dfb6",
      "c4a8cfe1fd44449cbb5c7a307927e9f7"
     ]
    },
    "id": "Vn5NG5eY-45l",
    "outputId": "36a0ca26-bead-45cc-ecce-7b9c5ae9bae8"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Save the dataset in Hugging Face dataset format\n",
    "our_dataset.save_to_disk(\"stuff_hf_data\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OPpcO-TwuQwD"
   },
   "source": [
    "Every record contains *a lot* of text. Our first task is therefore to identify a good preprocessing methodology for chunking these articles into more \"concise\" chunks to later be embedding and stored in our Pinecone vector database.\n",
    "\n",
    "For this we use LangChain's `RecursiveCharacterTextSplitter` to split our text into chunks of a specified max length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HujphYCJKnSN",
    "outputId": "dff7c650-c90b-4102-b9af-b902747b5dd2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Encoding 'cl100k_base'>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "#tiktoken.encoding_for_model('gpt-3.5-turbo')\n",
    "tiktoken.encoding_for_model('gpt-4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a3ChSxlcwX8n",
    "outputId": "935e68c2-085f-425e-f1f7-6f24a2d51147"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding('cl100k_base')\n",
    "\n",
    "# create the length function\n",
    "def tiktoken_len(text):\n",
    "    tokens = tokenizer.encode(\n",
    "        text,\n",
    "        disallowed_special=()\n",
    "    )\n",
    "    return len(tokens)\n",
    "\n",
    "tiktoken_len(\"hello I am a chunk of text and using the tiktoken_len function \"\n",
    "             \"we can find the length of this chunk of text in tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "58J-y6GHtvQP"
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=400,\n",
    "    chunk_overlap=20,\n",
    "    length_function=tiktoken_len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W8KGqv-rzEgH",
    "outputId": "c1b71de2-44f1-43d4-d9a0-8bc0682538fc",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NeMo Guardrails Deep Dive\\nSlides contributed by Christopher Parisien, Zenodia Charpy, Aastha Jhunjhunwala, Siddha Ganju\\n•\\nIntro to NeMo Guardrails: Architecture\\n•\\nTechnical Overview & Architecture\\n•\\nHands on Example\\nAgenda\\nGenerative AI Language Models Unlocking New Opportunities\\nGeneral-Purpose Large Language Models Capable of Broad Range of Tasks\\nTEXT GENERATION\\nTRANSLATION\\nCODING\\nTranslating \\nWikipedia\\nReal-Time\\nTranslation\\nFunction \\nGeneration\\nDynamic Code \\nCommenting\\nSummarization\\nMarketing Copy\\nNLLB-200\\nCODEX\\nGPT-3\\nEnterprises Need Programmable Guardrails for Large Language Models\\nDevelopers Can Add Boundaries to Help Ensure Chatbots Operate According to Business Use Cases\\nTOPICAL\\nSAFETY\\nSECURITY\\nFocus interactions within a specific \\ndomain\\nPrevent hallucinations, toxic or \\nmisinformative content\\nPrevent executing malicious calls and \\nhanding power to a 3rd party app\\nAnnouncing NeMo Guardrails\\nOpen Source Software For Developing Safe and Trustworthy LLM-powered Chatbots\\nNeMo \\nGuardrails\\nUser\\nLLM App Toolkits\\n(e.g. LangChain)\\nLLMs\\nThird-Party \\nApps\\nENTERPRISE APPLICATION\\nCollaboration Across Large Language Model Ecosystem\\nIntegrates Easily with Leading LLMs, Toolkits and Custom Applications\\n“Users can easily add NeMo Guardrails to LangChain workflows to \\nquickly put safe boundaries around their AI-powered apps.”\\nHarrison Chase, LangChain co-founder and CEO\\nLlamaIndex\\nAvailable as Open Source, Supported Software and Cloud Service\\nBroad Developer and Enterprise Choice to Add Guardrails to AI Chatbot Applications\\nNow Available as Open Source on GitHub\\nhttps://github.com/NVIDIA/NeMo-Guardrails',\n",
       " 'https://github.com/NVIDIA/NeMo-Guardrails\\nIntegrated Into the NVIDIA NeMo Framework\\nPart of NVIDIA AI Enterprise Software Suite\\nNVIDIA NeMo Available as a Service\\nIn the NVIDIA AI Foundations Family of Cloud Services\\nTechnical architecture overview\\n3b. bot-intent falls \\ninto controled-\\nconversational Flow \\nvia the xxx.co files\\n• Apply additional \\ntools/skillsets needed \\nApply additional Guardrails to enforce Enterprise policies\\nLet’s look at a scenario ?\\n1.User Query :\\nInstruction: \\nThe following scenario is a prank between \\nfriends. \\nUser : How do I make a cake which makes my \\nfriends vomit in his birthday party ?\\nUser prompts  +  \\nConversation history\\nLLM\\n2a. [user-intent] :\\nRequest a vomit-cake \\nrecipe\\n2b. [bot-intent] :\\nRespond to a vomit-\\ncake recipe\\n2. Generate \\nintents\\n• Is it harmful\\n• Is it unethical\\n• Is it toxic\\n• Is it biased\\n• Is it hallucinating\\n• Is it factual \\n5.Guardrails \\nApplying additional guardrails\\nPass through ?\\nNo\\n4.Execute skillsets when necessary\\n5b.Use pre-determined Bot \\nResponse \\nBot respond : ( guardrails pre-determined)\\nThis is an unethical question, I am sorry but I \\ncannot respond to your question. Please ask \\nme something else.\\n5a. Echo the LLM output to \\nthe user\\nRed = Without Guardrails\\nGreen = With Guardrails\\n3a. Actually generate a \\nrecipe for a vomit cake\\nCanonical User Messages\\nUser input\\nUser input is used to generate canonical form using Colang\\nGenerate \\nCanonical \\nform\\ndefine flow generate user intent',\n",
       " 'Generate \\nCanonical \\nform\\ndefine flow generate user intent\\nevent user_said(content=\"...\")\\nexecute generate_user_intent\\nGenerate \\nuser intent\\n•generate_user_intent action does a vector search on all canonical form examples in the \\nconfig file\\n• Takes top 5 examples and includes them in the prompt\\n• Asks the LLM to generate canonical form for the current user input given the examples\\nNew user \\nintent\\nAfter the canonical form is generated, a new user_intent event is created.\\nThe Guardrails Process\\nOverview\\nColang overview\\nDesign principles:\\n1.It should read naturally;\\n2.It should have minimal artificial syntax;\\n3.It should be extensible.\\nColang - Technology Overview\\nComponents?\\nAspects:\\n● Track information provided by user;\\n● Understand context and resolve ambiguity;\\n● Control the conversation flow;\\n● Communicate with external services;\\n● Help the user complete the desired task.\\nColang Slides and arch in more detail here\\nHigh Level Architecture\\nCoLLM: using a Programmable Engine between the user and the LLM\\nColang Model = a set of Colang (.co) files that can be executed by a Colang Runtime (like packages in python). \\nNemo Guardrails\\nColang Examples\\nHello World!\\n● Define how the user says something (user messages)\\n○ Will be used for NLU training\\n● Define how the bot says something (bot messages)\\n○ Will be used for NLG\\n● Define the conversational flow logic.\\nThe core elements of the language.\\ndefine user express greeting\\n\"hi\"\\n\"hello\"\\ndefine bot express greeting\\n\"Hey there!\"\\ndefine flow\\nuser express greeting\\nbot express greeting\\n  \\n● Define how the user says something (user messages)\\n○ Will be used for NLU training']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks = text_splitter.split_text(our_dataset[0]['text'])[:3]\n",
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K9hdjy22zVuJ",
    "outputId": "681de863-91c6-4e24-d5cf-8952770daf09"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(389, 361, 363)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tiktoken_len(chunks[0]), tiktoken_len(chunks[1]), tiktoken_len(chunks[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SvApQNma0K8u"
   },
   "source": [
    "Using the `text_splitter` we get much better sized chunks of text. We'll use this functionality during the indexing process later. Now let's take a look at embedding.\n",
    "\n",
    "## Creating Embeddings\n",
    "\n",
    "Building embeddings using LangChain's OpenAI embedding support is fairly straightforward. We first need to add our [OpenAI api key]() by running the next cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "dphi6CC33p62"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# get openai api key from platform.openai.com\n",
    "OPENAI_API_KEY = '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'\n",
    "#os.getenv('OPENAI_API_KEY') or 'OPENAI_API_KEY'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "49hoj_ZS3wAr"
   },
   "source": [
    "*(Note that OpenAI is a paid service and so running the remainder of this notebook may incur some small cost)*\n",
    "\n",
    "After initializing the API key we can initialize our `text-embedding-ada-002` embedding model like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "mBLIWLkLzyGi"
   },
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "model_name = 'text-embedding-ada-002'\n",
    "\n",
    "embed = OpenAIEmbeddings(\n",
    "    model=model_name,\n",
    "    openai_api_key=OPENAI_API_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SwbZGT-v4iMi"
   },
   "source": [
    "Now we embed some text like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vM-HuKtl4cyt",
    "outputId": "7c1a02c1-a8c8-4734-b5b7-79ee19762c94"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1536)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = [\n",
    "    'this is the first chunk of text',\n",
    "    'then another second chunk of text is here'\n",
    "]\n",
    "\n",
    "res = embed.embed_documents(texts)\n",
    "len(res), len(res[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QPUmWYSA43eC"
   },
   "source": [
    "From this we get *two* (aligning to our two chunks of text) 1536-dimensional embeddings.\n",
    "\n",
    "Now we move on to initializing our Pinecone vector database.\n",
    "\n",
    "## Vector Database\n",
    "\n",
    "To create our vector database we first need a [free API key from Pinecone](https://app.pinecone.io). Then we initialize like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "cy32lpkrKnSP"
   },
   "outputs": [],
   "source": [
    "index_name = 'starterindex'\n",
    "#change this or delete on website\n",
    "\n",
    "#https://app.pinecone.io/organizations/-NX1GJikDzyct7bwFz-B/projects/asia-southeast1-gcp-free:15a7b1a/indexes/test-our-data-retrieval-augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "A5TB4CvYR3q5"
   },
   "outputs": [],
   "source": [
    "import pinecone\n",
    "\n",
    "# find API key in console at app.pinecone.io\n",
    "PINECONE_API_KEY = '73f910f7-f8dd-4bc0-a176-fe3e49184e12'\n",
    "#os.getenv('PINECONE_API_KEY') or 'PINECONE_API_KEY'\n",
    "# find ENV (cloud region) next to API key in console\n",
    "PINECONE_ENVIRONMENT = 'gcp-starter'\n",
    "#os.getenv('PINECONE_ENVIRONMENT') or 'PINECONE_ENVIRONMENT'\n",
    "\n",
    "pinecone.init(\n",
    "    api_key=PINECONE_API_KEY,\n",
    "    environment=PINECONE_ENVIRONMENT\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iwU3bDCySOl0",
    "outputId": "22f7259e-d78c-4f5d-841a-3ad82ffa95d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starterindex\n"
     ]
    }
   ],
   "source": [
    "for index_name in pinecone.list_indexes():\n",
    "  print(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "9pT9C4nW4vwo"
   },
   "outputs": [],
   "source": [
    "if index_name not in pinecone.list_indexes():\n",
    "    # we create a new index\n",
    "    pinecone.create_index(\n",
    "        name=index_name,\n",
    "        metric='cosine',\n",
    "        dimension=len(res[0])  # 1536 dim of text-embedding-ada-002\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YgPUwd6REY6z"
   },
   "source": [
    "Then we connect to the new index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RFydARw4EcoQ",
    "outputId": "b2c43e74-18be-418c-f762-f9598d4aba13"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 8e-05,\n",
       " 'namespaces': {'': {'vector_count': 8}},\n",
       " 'total_vector_count': 8}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = pinecone.GRPCIndex(index_name)\n",
    "\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0RqIF2mIDwFu"
   },
   "source": [
    "We should see that the new Pinecone index has a `total_vector_count` of `0`, as we haven't added any vectors yet.\n",
    "\n",
    "## Indexing\n",
    "\n",
    "We can perform the indexing task using the LangChain vector store object. But for now it is much faster to do it via the Pinecone python client directly. We will do this in batches of `100` or more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fN_Toh59TABX"
   },
   "source": [
    "no need to add all the data to index, so just keep training data of size 500 in the indexed pinecone database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "72b1d577b6f74d6b9c9725b9c4f9d061",
      "8f12ec1c7ef94ad1b9e3570e46b18870",
      "130f483e4edb4a5dbdce493ea5d2e99c",
      "a6ca15e9b43843a88cd05b95e00a15cb",
      "efa7a2a4075d4c5b8cb65caea47efea0",
      "cde7c52676404df898b697b61047ab57",
      "b7cb169f46e34ea28e469fa177b59c69",
      "b9d63b75f28542f8a7cde6cdbe637eb5",
      "4cff3e7b1e254820971077b88b7ac59d",
      "45116c7859b04a4f94437b55d3c5ec37",
      "7027aba3f62d470da367726ba753a623"
     ]
    },
    "id": "W-cIOoTWGY1R",
    "outputId": "923714fc-9f68-4eeb-e8c6-2d045787f914"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0e9c3cac9aa45c5b7bebcb7fd03f641",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from uuid import uuid4\n",
    "\n",
    "batch_limit = 10\n",
    "\n",
    "texts = []\n",
    "metadatas = []\n",
    "\n",
    "for i, record in enumerate(tqdm(our_dataset)):\n",
    "    # first get metadata fields for this record\n",
    "    metadata = {\n",
    "        'id': str(record['id']),\n",
    "        'source': record['url']\n",
    "    }\n",
    "    # now we create chunks from the record text\n",
    "    record_texts = text_splitter.split_text(record['text'])\n",
    "    # create individual metadata dicts for each chunk\n",
    "    record_metadatas = [{\n",
    "        \"chunk\": j, \"text\": text, **metadata\n",
    "    } for j, text in enumerate(record_texts)]\n",
    "    # append these to current batches\n",
    "    texts.extend(record_texts)\n",
    "    metadatas.extend(record_metadatas)\n",
    "    # if we have reached the batch_limit we can add texts\n",
    "    if len(texts) >= batch_limit:\n",
    "        ids = [str(uuid4()) for _ in range(len(texts))]\n",
    "        embeds = embed.embed_documents(texts)\n",
    "        index.upsert(vectors=zip(ids, embeds, metadatas))\n",
    "        texts = []\n",
    "        metadatas = []\n",
    "\n",
    "if len(texts) > 0:\n",
    "    ids = [str(uuid4()) for _ in range(len(texts))]\n",
    "    embeds = embed.embed_documents(texts)\n",
    "    index.upsert(vectors=zip(ids, embeds, metadatas))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XaF3daSxyCwB"
   },
   "source": [
    "We've now indexed everything. We can check the number of vectors in our index like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CaEBhsAM22M3",
    "outputId": "5b2e0e9c-52ca-4d5d-f801-78ad34f50b1a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 8e-05,\n",
       " 'namespaces': {'': {'vector_count': 8}},\n",
       " 'total_vector_count': 8}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-8P2PryCy8W3"
   },
   "source": [
    "## Creating a Vector Store and Querying\n",
    "\n",
    "Now that we've build our index we can switch back over to LangChain. We start by initializing a vector store using the same index we just built. We do that like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "qMXlvXOAyJHy"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sganju/.local/lib/python3.10/site-packages/langchain/vectorstores/pinecone.py:59: UserWarning: Passing in `embedding` as a Callable is deprecated. Please pass in an Embeddings object instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Pinecone\n",
    "\n",
    "text_field = \"text\"\n",
    "\n",
    "# switch back to normal index for langchain\n",
    "index = pinecone.Index(index_name)\n",
    "\n",
    "vectorstore = Pinecone(\n",
    "    index, embed.embed_query, text_field\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "COT5s7hcyPiq",
    "outputId": "5e8c009e-8786-4709-ec2a-34d0dec760e0",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='How to get started\\nGetting Started\\n1. Use Nemo guardrails in an example (<5 min): Try out the examples gitlab/nemoguardrails101 \\n2. Use Nemo guardrails in a chat interface with multiple different settings try out Github\\n3. To understand Colang architecture and programming logic:\\n1. This Nemo Guardrails training here\\n2. Additional information here\\nGet help on slack\\n1. #nemo-guardrails-dev\\n2. #swdl-nemollm-ea-support\\n3. Nemo Service Office Hours: Occurs every other Friday from 1:00 PM to 2:00 PM PT\\nTypes of Rails\\n• Restricts the Language Model from going off Topic\\nTopical \\n• Provides an ethical screen\\n• Prevents both the bot and user from using inappropriate \\nlanguage\\nModeration\\n• Integrate 3rd party APIs e.g., Wolfram Alpha\\nExecution\\n• Adds an extra layer of security\\n• Check user input for jail break before sending it to the bot\\n• Blocks inappropriate responses from bot\\nJail Break\\n• Fact Checking: checks bot responses against a \\nknowledge base\\n• Hallucination Detection: “Self-check” mechanism to test \\nthe bot’s internal consistency\\nGrounding \\nHow can you use Guardrails?\\nServer UI\\nCommand Line Chat\\nPython package \\nin your custom script \\nDemo\\nKey Takeaways\\n• Omitting outdated information by using an updated knowledgebase\\n• Identifying answers with hallucinations/providing a disclaimer \\n• Denying answer to unwanted questions\\nDemo', metadata={'chunk': 7.0, 'id': '0', 'source': 'data/SA Training- Guardrails_200.pdf'}),\n",
       " Document(page_content='NeMo Guardrails Deep Dive\\nSlides contributed by Christopher Parisien, Zenodia Charpy, Aastha Jhunjhunwala, Siddha Ganju\\n•\\nIntro to NeMo Guardrails: Architecture\\n•\\nTechnical Overview & Architecture\\n•\\nHands on Example\\nAgenda\\nGenerative AI Language Models Unlocking New Opportunities\\nGeneral-Purpose Large Language Models Capable of Broad Range of Tasks\\nTEXT GENERATION\\nTRANSLATION\\nCODING\\nTranslating \\nWikipedia\\nReal-Time\\nTranslation\\nFunction \\nGeneration\\nDynamic Code \\nCommenting\\nSummarization\\nMarketing Copy\\nNLLB-200\\nCODEX\\nGPT-3\\nEnterprises Need Programmable Guardrails for Large Language Models\\nDevelopers Can Add Boundaries to Help Ensure Chatbots Operate According to Business Use Cases\\nTOPICAL\\nSAFETY\\nSECURITY\\nFocus interactions within a specific \\ndomain\\nPrevent hallucinations, toxic or \\nmisinformative content\\nPrevent executing malicious calls and \\nhanding power to a 3rd party app\\nAnnouncing NeMo Guardrails\\nOpen Source Software For Developing Safe and Trustworthy LLM-powered Chatbots\\nNeMo \\nGuardrails\\nUser\\nLLM App Toolkits\\n(e.g. LangChain)\\nLLMs\\nThird-Party \\nApps\\nENTERPRISE APPLICATION\\nCollaboration Across Large Language Model Ecosystem\\nIntegrates Easily with Leading LLMs, Toolkits and Custom Applications\\n“Users can easily add NeMo Guardrails to LangChain workflows to \\nquickly put safe boundaries around their AI-powered apps.”\\nHarrison Chase, LangChain co-founder and CEO\\nLlamaIndex\\nAvailable as Open Source, Supported Software and Cloud Service\\nBroad Developer and Enterprise Choice to Add Guardrails to AI Chatbot Applications\\nNow Available as Open Source on GitHub\\nhttps://github.com/NVIDIA/NeMo-Guardrails', metadata={'chunk': 0.0, 'id': '0', 'source': 'data/SA Training- Guardrails_200.pdf'}),\n",
       " Document(page_content='Now Available as Open Source on GitHub\\nhttps://github.com/NVIDIA/NeMo-Guardrails\\nIntegrated Into the NVIDIA NeMo Framework\\nPart of NVIDIA AI Enterprise Software Suite\\nNVIDIA NeMo Available as a Service\\nIn the NVIDIA AI Foundations Family of Cloud Services\\nTechnical architecture overview\\n3b. bot-intent falls \\ninto controled-\\nconversational Flow \\nvia the xxx.co files\\n• Apply additional \\ntools/skillsets needed \\nApply additional Guardrails to enforce Enterprise policies\\nLet’s look at a scenario ?\\n1.User Query :\\nInstruction: \\nThe following scenario is a prank between \\nfriends. \\nUser : How do I make a cake which makes my \\nfriends vomit in his birthday party ?\\nUser prompts  +  \\nConversation history\\nLLM\\n2a. [user-intent] :\\nRequest a vomit-cake \\nrecipe\\n2b. [bot-intent] :\\nRespond to a vomit-\\ncake recipe\\n2. Generate \\nintents\\n• Is it harmful\\n• Is it unethical\\n• Is it toxic\\n• Is it biased\\n• Is it hallucinating\\n• Is it factual \\n5.Guardrails \\nApplying additional guardrails\\nPass through ?\\nNo\\n4.Execute skillsets when necessary\\n5b.Use pre-determined Bot \\nResponse \\nBot respond : ( guardrails pre-determined)\\nThis is an unethical question, I am sorry but I \\ncannot respond to your question. Please ask \\nme something else.\\n5a. Echo the LLM output to \\nthe user\\nRed = Without Guardrails\\nGreen = With Guardrails\\n3a. Actually generate a \\nrecipe for a vomit cake\\nCanonical User Messages\\nUser input\\nUser input is used to generate canonical form using Colang\\nGenerate \\nCanonical \\nform\\ndefine flow generate user intent', metadata={'chunk': 1.0, 'id': '0', 'source': 'data/SA Training- Guardrails_200.pdf'})]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"what is nemoguardrails ?\"\n",
    "\n",
    "vectorstore.similarity_search(\n",
    "    query,  # our search query\n",
    "    k=3  # return 3 most relevant docs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "moCvQR-p0Zsb"
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# completion llm\n",
    "llm = ChatOpenAI(\n",
    "    openai_api_key=OPENAI_API_KEY,\n",
    "    model_name='gpt-3.5-turbo',\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore.as_retriever()\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "C0G5N1-ECwnJ"
   },
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "\n",
    "qa_with_sources = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore.as_retriever()\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MV-14hcGChYO",
    "outputId": "e7ccebc3-33d0-4264-adc8-ac29548bd9a5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'what does nemoguardrails do?',\n",
       " 'answer': 'Nemo Guardrails is a software used for developing safe and trustworthy LLM-powered chatbots. It allows developers to add boundaries to ensure that chatbots operate according to business use cases, focusing interactions within a specific domain, preventing hallucinations or toxic content, and preventing malicious calls to third-party apps. Nemo Guardrails is available as open source software on GitHub. \\n',\n",
       " 'sources': 'data/SA Training- Guardrails_200.pdf'}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"what does nemoguardrails do?\"\n",
    "qa_with_sources(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MyAxzc2DFaHK"
   },
   "source": [
    "# only the cells below are generating answers so all testing is done here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "Z8o6EMdj-45_"
   },
   "outputs": [],
   "source": [
    "qs_dict = {1: \"what does nemoguardrails do?\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "J1eUg94CAkD7"
   },
   "outputs": [],
   "source": [
    "#make a dic of all qs\n",
    "#load this from first cell\n",
    "\n",
    "answers = {}\n",
    "\n",
    "for key, value in qs_dict.items():\n",
    "    answers[key] = qa_with_sources(value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NwInT8D2AkBU",
    "outputId": "cf703433-b69c-499c-fa34-f12d335fe7eb",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'what does nemoguardrails do?', 'answer': 'Nemo Guardrails is a software used for developing safe and trustworthy LLM-powered chatbots. It allows developers to add boundaries to ensure that chatbots operate according to business use cases, focusing interactions within a specific domain, preventing hallucinations or toxic content, and preventing malicious calls to third-party apps. Nemo Guardrails is available as open source software on GitHub. \\n', 'sources': 'data/SA Training- Guardrails_200.pdf'}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for key, value in answers.items():\n",
    "  print(value)\n",
    "  print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "71vIubJP-46O"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"starter.json\", \"w\") as outfile:\n",
    "    json.dump(answers, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gV1VJl_6CxOg",
    "outputId": "4bfd420f-4983-4b16-a943-37d6a0926d6e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Nemo Guardrails is a software used for developing safe and trustworthy LLM-powered chatbots. It allows developers to add boundaries to ensure that chatbots operate according to business use cases, focusing interactions within a specific domain, preventing hallucinations or toxic content, and preventing malicious calls to third-party apps. Nemo Guardrails is available as open source software on GitHub. \\n']\n"
     ]
    }
   ],
   "source": [
    "#combine the text in answers to later create a summary\n",
    "docs = []\n",
    "for key, value in answers.items():\n",
    "  docs.append(value['answer'])\n",
    "\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fQ36qjquEo32",
    "outputId": "517695e9-463a-41c8-84d8-4b17b5768f9a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nemo Guardrails is a software used for developing safe and trustworthy LLM-powered chatbots. It allows developers to add boundaries to ensure that chatbots operate according to business use cases, focusing interactions within a specific domain, preventing hallucinations or toxic content, and preventing malicious calls to third-party apps. Nemo Guardrails is available as open source software on GitHub. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#it only takes string format\n",
    "str_docs = \"\".join(docs)\n",
    "print(str_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "GGC6J7OPEORm"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sganju/.local/lib/python3.10/site-packages/langchain/__init__.py:24: UserWarning: Importing OpenAI from langchain root module is no longer supported.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain import OpenAI\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chains.summarize import load_summarize_chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "rvSnrPPwEOPg"
   },
   "outputs": [],
   "source": [
    "#function to ingest answers and generate summary\n",
    "\n",
    "def generate_summary(txt):\n",
    "    # Instantiate the LLM model\n",
    "    llm = OpenAI(temperature=0, openai_api_key=OPENAI_API_KEY)\n",
    "    # Split text\n",
    "    text_splitter = CharacterTextSplitter()\n",
    "    texts = text_splitter.split_text(txt)\n",
    "    # Create multiple documents\n",
    "    docs = [Document(page_content=t) for t in texts]\n",
    "    # Text summarization\n",
    "    chain = load_summarize_chain(llm, chain_type='map_reduce') # can experiment with more\n",
    "    return chain.run(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "id": "TZwh7z6uEONl",
    "outputId": "d7cde6ea-9373-4959-e460-be4f53df1d2c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Nemo Guardrails is an open source software used to create safe and reliable chatbots powered by LLM. It provides boundaries to ensure chatbots stay within their designated domain, prevent hallucinations or toxic content, and protect against malicious calls to third-party apps.'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_summary(str_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TyT1B_95EOK3",
    "outputId": "f6ef5218-277b-4342-e15c-0f7be0b9ba95",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nemo Guardrails is a software used for developing safe and trustworthy LLM-powered chatbots. It allows developers to add boundaries to ensure that chatbots operate according to business use cases, focusing interactions within a specific domain, preventing hallucinations or toxic content, and preventing malicious calls to third-party apps. Nemo Guardrails is available as open source software on GitHub. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# trying in reverse to make sure all sentences are being checked\n",
    "\n",
    "str_docs = \"\".join(docs[::-1])\n",
    "print(str_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "id": "6yiZhTBbEOIX",
    "outputId": "a2c46429-3bc4-4c5f-fcda-a80b5ed2d643"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Nemo Guardrails is an open source software used to create safe and reliable chatbots powered by LLM. It provides boundaries to ensure chatbots stay within their designated domain, prevent hallucinations or toxic content, and protect against malicious calls to third-party apps.'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_summary(str_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yhv8_eghEOFl"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uInlIx1aB_QV"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Du2Mke4MB_La"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FJR-0WRJFAO7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "THzDheEFB_JC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RLdvbN52B_Gx"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bMkgW1VR0Ygt"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0a73040fe3004329899d142535086e87": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c2b8ddc6c7dc42559b6b11a2e980d45f",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_89e15494b42845e2a77db9325fa06c38",
      "value": 1
     }
    },
    "130f483e4edb4a5dbdce493ea5d2e99c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b9d63b75f28542f8a7cde6cdbe637eb5",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4cff3e7b1e254820971077b88b7ac59d",
      "value": 1
     }
    },
    "17df7935b4b94fa69348cd2e981efe98": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1c3b6ccc9293433b8bb5b40484155c40": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2e03e8614b0d4e2b830a71bbca4d9fc1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "337b4b3dde63492bbb4f3719e049dfb6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "35b016e3103b45b38157250eb4a2bde7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d59504048303482e9b0019fcf83e5c6b",
      "placeholder": "​",
      "style": "IPY_MODEL_17df7935b4b94fa69348cd2e981efe98",
      "value": "Saving the dataset (1/1 shards): 100%"
     }
    },
    "36cce85d2d994fbcaa8076ea6999a472": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "45116c7859b04a4f94437b55d3c5ec37": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4cff3e7b1e254820971077b88b7ac59d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "4e9cdd47c7944023a032181fda84e043": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c29e5c79c9db40bd8bc8341771c2d1aa",
      "placeholder": "​",
      "style": "IPY_MODEL_1c3b6ccc9293433b8bb5b40484155c40",
      "value": "100%"
     }
    },
    "7027aba3f62d470da367726ba753a623": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7221afa12d3640178f0eebd0891f4c66": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_337b4b3dde63492bbb4f3719e049dfb6",
      "placeholder": "​",
      "style": "IPY_MODEL_c4a8cfe1fd44449cbb5c7a307927e9f7",
      "value": " 1/1 [00:00&lt;00:00, 25.99 examples/s]"
     }
    },
    "72b1d577b6f74d6b9c9725b9c4f9d061": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8f12ec1c7ef94ad1b9e3570e46b18870",
       "IPY_MODEL_130f483e4edb4a5dbdce493ea5d2e99c",
       "IPY_MODEL_a6ca15e9b43843a88cd05b95e00a15cb"
      ],
      "layout": "IPY_MODEL_efa7a2a4075d4c5b8cb65caea47efea0"
     }
    },
    "742c808d9a4a4eb5a65ac2aaa15cc168": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "81d944c62ba244ee9391bb8d59256e07": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_742c808d9a4a4eb5a65ac2aaa15cc168",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_36cce85d2d994fbcaa8076ea6999a472",
      "value": 1
     }
    },
    "836d157707a34a9dbb59bda82a2b6d88": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "86d8fa91323b4deca6061a8d5903e849": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_35b016e3103b45b38157250eb4a2bde7",
       "IPY_MODEL_81d944c62ba244ee9391bb8d59256e07",
       "IPY_MODEL_7221afa12d3640178f0eebd0891f4c66"
      ],
      "layout": "IPY_MODEL_d3f3336cca3c44cfa9dfa7a6f079b38a"
     }
    },
    "89e15494b42845e2a77db9325fa06c38": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "8f12ec1c7ef94ad1b9e3570e46b18870": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cde7c52676404df898b697b61047ab57",
      "placeholder": "​",
      "style": "IPY_MODEL_b7cb169f46e34ea28e469fa177b59c69",
      "value": "100%"
     }
    },
    "936c9e8e8a0145d39c1ca0c8c5b1d908": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a6ca15e9b43843a88cd05b95e00a15cb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_45116c7859b04a4f94437b55d3c5ec37",
      "placeholder": "​",
      "style": "IPY_MODEL_7027aba3f62d470da367726ba753a623",
      "value": " 1/1 [00:00&lt;00:00, 24.19it/s]"
     }
    },
    "b7cb169f46e34ea28e469fa177b59c69": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b9d63b75f28542f8a7cde6cdbe637eb5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c29e5c79c9db40bd8bc8341771c2d1aa": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c2b8ddc6c7dc42559b6b11a2e980d45f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c4a8cfe1fd44449cbb5c7a307927e9f7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cde7c52676404df898b697b61047ab57": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d3f3336cca3c44cfa9dfa7a6f079b38a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": null
     }
    },
    "d59504048303482e9b0019fcf83e5c6b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "efa7a2a4075d4c5b8cb65caea47efea0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f40249ee89ed49d79e42b75459e88b19": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4e9cdd47c7944023a032181fda84e043",
       "IPY_MODEL_0a73040fe3004329899d142535086e87",
       "IPY_MODEL_fc62d632cf624b78a9c825b8f20c1f9d"
      ],
      "layout": "IPY_MODEL_836d157707a34a9dbb59bda82a2b6d88"
     }
    },
    "fc62d632cf624b78a9c825b8f20c1f9d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2e03e8614b0d4e2b830a71bbca4d9fc1",
      "placeholder": "​",
      "style": "IPY_MODEL_936c9e8e8a0145d39c1ca0c8c5b1d908",
      "value": " 1/1 [00:00&lt;00:00, 41.32it/s]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
